# Adversarial Attacks and Defense Mechanisms in Deep Learning Models

## Project Overview

This project was done as part of Data Science course of M2 IASD. It explores adversarial attacks and defense mechanisms in deep learning models, focusing on improving both clean accuracy and robustness. 

In this project, we explore adversarial attacks and defense mechanisms in deep learning models, focusing on improving both clean accuracy
and robustness. Initially, we implemented FGSM (Fast Gradient Sign Method) and PGD (Projected Gradient Descent) adversarial training
to enhance the model’s resilience to attacks. Building on this foundation, we introduced AutoAttack as a key metric to evaluate the
model’s robustness. To further improve the accuracy-robustness trade-off, we incorporated MixedNUTS, a training-free approach that
combines standard and robust classifiers through nonlinearly mixed outputs.

## Table of Contents

1. [Introduction](#introduction)
2. [AutoAttack Accuracy with Adversarial Training](#autoattack-accuracy-with-adversarial-training)
3. [MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers](#mixednuts-training-free-accuracy-robustness-balance-via-nonlinearly-mixed-classifiers)
4. [Optimization](#optimization)
5. [Experiments](#experiments)
6. [Conclusion](#conclusion)
7. [Bibliography](#bibliography)

## Team Members
1. Kanupriya Jain
2. Mohamed Ali
3. Anna Krysta
